{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.6485, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.1626, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.7027, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.4229, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.1231, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.0263, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7293, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7074, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "vet IMr'bV$Ollackze hit aind thr, hith$!.\n",
      "QX$Gl CHorivinsimalS.\n",
      "-ine sis?imLithinR?USoDU,RSpiA-inYzeinit f fln:, rw t$NI'JFo te I! ghe nkUMgellYpa!\n",
      "nof s,MZYP gea!\n",
      "Wot hjBuFRy Jaman ve.\n",
      "TotrgHowes$F-pe sheaI'dd, nYKaNCIiorvil.\n",
      "IV; icNo'o aumed s; wlrerr non;brJifrus ?he noco'dco h s golf sthiszKFir:D$LIfixr:\n",
      "A:EYo.\n",
      "T-\n",
      "IINDYef itTheyO:Comyway bod s:\n",
      "CErp : avealiavot,&VMCKang!BOl d\n",
      "T' genVJates, :zR:CEY ff?fulvNuris CIndO: wzo a -vPve, cofanbPos st\n",
      "WeaSTheaves.:\n",
      "Akenremeallasu wnoitRat HAKStn cai\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 3500\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "def encode(word):\n",
    "    stoi = {sorted(list(all_characters))[i]:i for i in range(len(all_characters))}\n",
    "    return [stoi[letter] for letter in word]\n",
    "\n",
    "def decode(list_of_numbers):\n",
    "    itos = {i:sorted(list(all_characters))[i] for i in range(len(all_characters))}\n",
    "    return ''.join([itos[i] for i in list_of_numbers])\n",
    "\n",
    "def get_batch(batch_size, block_size, data):\n",
    "    idxs = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
    "    xs = torch.stack([train[idx:idx+block_size] for idx in idxs])\n",
    "    ys = torch.stack([train[idx+1:idx+block_size+1] for idx in idxs])\n",
    "    xs, ys = xs.to(device), ys.to(device)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.guess_next_letter_matrix = torch.nn.Embedding(number_of_characters,number_of_characters)\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        logits = self.guess_next_letter_matrix(x)\n",
    "        logits_reshaped = logits.view(logits.shape[0] * logits.shape[1], logits.shape[2])\n",
    "        target = y.view(logits.shape[0] * logits.shape[1])\n",
    "        loss = F.cross_entropy(logits_reshaped,target)\n",
    "        return logits_reshaped, loss\n",
    "    \n",
    "    def generate(self, x, how_many_tokens):\n",
    "        for _ in range(how_many_tokens):\n",
    "            last_letter = x[:,-1].view(x.shape[0])\n",
    "            logits = self.guess_next_letter_matrix(last_letter)\n",
    "            next_letter_probability_dist = F.softmax(logits, dim = 1)\n",
    "            next_letter_guess = torch.multinomial(next_letter_probability_dist,1)\n",
    "            x = torch.cat((x,next_letter_guess), dim=1)\n",
    "        return x\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as text:\n",
    "    data = text.read()\n",
    "all_characters = set(data)\n",
    "number_of_characters = len(all_characters)\n",
    "train = torch.tensor(encode(data)[:int(0.9*len(data))], dtype = torch.long)\n",
    "test = torch.tensor(encode(data)[int(0.9 * len(data)):], dtype = torch.long)\n",
    "\n",
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "for step in range(max_iters):\n",
    "    xb, yb = get_batch(batch_size, block_size, train)\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 500 == 0:\n",
    "        print('Loss:',loss)\n",
    "\n",
    "print('Loss:', loss)\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "print(decode(model.generate(context, how_many_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
    "        q = self.query(x) #B,T,head\n",
    "        k = self.key(x)   #B,T,head\n",
    "        v = self.value(x) #B,T,head\n",
    "        wei = q @ k.transpose(-2,-1) * C ** -0.5#B,T,T\n",
    "        tril = torch.tril(torch.ones(T,T)) #T,T\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) #B,T,T\n",
    "        wei = F.softmax(wei,dim=-1)#B,T,T\n",
    "        output = wei @ v #B,T,head\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1) \n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(number_of_characters,n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.sa_heads = MultiHeadAttention(4,n_emb//4)\n",
    "        self.lm_head = nn.Linear(n_emb, number_of_characters)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        token_embedding = self.embedding(x) # B, T, n_emb\n",
    "        position_encoding = self.position_embedding_table(torch.arange(x.shape[1], device = device)) # T,C\n",
    "        x = token_embedding + position_encoding # new dimension of 1 is added, and it's broadcasted up the batches\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "        logits_reshaped = logits.view(logits.shape[0] * logits.shape[1],-1)\n",
    "        target = y.view(logits.shape[0] * logits.shape[1])\n",
    "        loss = F.cross_entropy(logits_reshaped,target)\n",
    "        return logits_reshaped, loss\n",
    "    \n",
    "    def forward_pass_only(self, x):\n",
    "        token_embedding = self.embedding(x) # B, T, n_emb\n",
    "        position_encoding = self.position_embedding_table(torch.arange(x.shape[1], device = device)) # T,C\n",
    "        x = token_embedding + position_encoding # new dimension of 1 is added, and it's broadcasted up the batches\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def generate(self, x, how_many_tokens):\n",
    "        for _ in range(how_many_tokens):\n",
    "            context = x[:, -block_size:]\n",
    "            logits = self.forward_pass_only(context)\n",
    "            next_letter_probability_dist = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_letter_guess = torch.multinomial(next_letter_probability_dist, 1)\n",
    "            x = torch.cat((x, next_letter_guess), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "xb, yb = get_batch(batch_size, block_size, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.2357, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6528, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4865, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5098, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3091, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3195, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4876, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3926, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Whis as hath!\n",
      "I fo\n",
      "anit.\n",
      "\n",
      "Whofrem do.\n",
      "\n",
      "IGWo, ha, ime aks Aivowe a, Cgthasat his,\n",
      "Bot hearke, heied Ird fo.\n",
      "\n",
      "DBour to bree. sy wor sok hetucthachirich sitoth. wherve ay shilld thoul ore,\n",
      "DI men gabnd ckencthine thithon'd herss irto eitleld hou yo mmeeve-dee me, man, masce-Bu othe em Has wan.\n",
      "UESRKEIR:\n",
      "Fat axthimy my lant cod theme hig yno oudre?\n",
      "\n",
      "CAnerit athe,'d, yefiad hy hreadabr se himealn.\n",
      "\n",
      "ARYeivilllorbrur:\n",
      "He ppluead nd O:\n",
      "Fof thilderedint loour ief, Aadnou pith home, thatwat nre fo ngouce,\n"
     ]
    }
   ],
   "source": [
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "for step in range(max_iters):\n",
    "    xb, yb = get_batch(batch_size, block_size, train)\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 500 == 0:\n",
    "        print('Loss:',loss)\n",
    "\n",
    "print('Loss:', loss)\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "print(decode(model.generate(context, how_many_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHeaded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
    "        q = self.query(x) #B,T,head\n",
    "        k = self.key(x)   #B,T,head\n",
    "        v = self.value(x) #B,T,head\n",
    "        wei = q @ k.transpose(-2,-1) * C ** -0.5 #B,T,T\n",
    "        tril = torch.tril(torch.ones(T,T)) #T,T\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) #B,T,T\n",
    "        wei = F.softmax(wei,dim=-1)#B,T,T\n",
    "        output = wei @ v #B,T,head\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1) \n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(number_of_characters,n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.sa_heads = MultiHeadAttention(4,n_emb//4)\n",
    "        self.lm_head = nn.Linear(n_emb, number_of_characters)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        token_embedding = self.embedding(x) # B, T, n_emb\n",
    "        position_encoding = self.position_embedding_table(torch.arange(x.shape[1], device = device)) # T,C\n",
    "        x = token_embedding + position_encoding # new dimension of 1 is added, and it's broadcasted up the batches\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "        logits_reshaped = logits.view(logits.shape[0] * logits.shape[1],-1)\n",
    "        target = y.view(logits.shape[0] * logits.shape[1])\n",
    "        loss = F.cross_entropy(logits_reshaped,target)\n",
    "        return logits_reshaped, loss\n",
    "    \n",
    "    def forward_pass_only(self, x):\n",
    "        token_embedding = self.embedding(x) # B, T, n_emb\n",
    "        position_encoding = self.position_embedding_table(torch.arange(x.shape[1], device = device)) # T,C\n",
    "        x = token_embedding + position_encoding # new dimension of 1 is added, and it's broadcasted up the batches\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def generate(self, x, how_many_tokens):\n",
    "        for _ in range(how_many_tokens):\n",
    "            context = x[:, -block_size:]\n",
    "            logits = self.forward_pass_only(context)\n",
    "            next_letter_probability_dist = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_letter_guess = torch.multinomial(next_letter_probability_dist, 1)\n",
    "            x = torch.cat((x, next_letter_guess), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "xb, yb = get_batch(batch_size, block_size, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.2477, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.6790, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.7075, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2938, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4916, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4201, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2491, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3200, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "WO:\n",
      "Thereely aik you what!\n",
      "Yo:\n",
      "Thallisest tou forg yusith wesnd to yo wot ppmy utlit nier the hing his the tercor dis?- why nom.\n",
      "\n",
      "Whayoun sur, nowh yo athe auty,\n",
      "Whes hin! sevendpe muse:\n",
      "It.\n",
      "\n",
      "Grithis.\n",
      "SLARCILAS:\n",
      "VThamiver thathe swe noth wing ecechelabe hay pom I difermape hele QESIUSUETEULIENVIII by sen; an com acof all:\n",
      "Thim lave, figaimn.\n",
      "\n",
      "Lathspad obursple allancerdth empilt to ite dep me ay thiche, fer it mom toe of athithste outh he his taes pre.\n",
      "BUEARIDULCAR:\n",
      "Bus so tould:\n",
      "If trinkn thich\n"
     ]
    }
   ],
   "source": [
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "for step in range(max_iters):\n",
    "    xb, yb = get_batch(batch_size, block_size, train)\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 500 == 0:\n",
    "        print('Loss:',loss)\n",
    "\n",
    "print('Loss:', loss)\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "print(decode(model.generate(context, how_many_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
    "        q = self.query(x) #B,T,head\n",
    "        k = self.key(x)   #B,T,head\n",
    "        v = self.value(x) #B,T,head\n",
    "        wei = q @ k.transpose(-2,-1) * C ** -0.5#B,T,T\n",
    "        tril = torch.tril(torch.ones(T,T)) #T,T\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) #B,T,T\n",
    "        wei = F.softmax(wei,dim=-1)#B,T,T\n",
    "        output = wei @ v #B,T,head\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1) \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb,n_emb),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(number_of_characters,n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.sa_heads = MultiHeadAttention(4,n_emb//4)\n",
    "        self.ffwd = FeedForward(n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, number_of_characters)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        token_embedding = self.embedding(x) # B, T, n_emb\n",
    "        position_encoding = self.position_embedding_table(torch.arange(x.shape[1], device = device)) # T,C\n",
    "        x = token_embedding + position_encoding # new dimension of 1 is added, and it's broadcasted up the batches\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x)\n",
    "        logits_reshaped = logits.view(logits.shape[0] * logits.shape[1],-1)\n",
    "        target = y.view(logits.shape[0] * logits.shape[1])\n",
    "        loss = F.cross_entropy(logits_reshaped,target)\n",
    "        return logits_reshaped, loss\n",
    "    \n",
    "    def forward_pass_only(self, x):\n",
    "        token_embedding = self.embedding(x) # B, T, n_emb\n",
    "        position_encoding = self.position_embedding_table(torch.arange(x.shape[1], device = device)) # T,C\n",
    "        x = token_embedding + position_encoding # new dimension of 1 is added, and it's broadcasted up the batches\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def generate(self, x, how_many_tokens):\n",
    "        for _ in range(how_many_tokens):\n",
    "            context = x[:, -block_size:]\n",
    "            logits = self.forward_pass_only(context)\n",
    "            next_letter_probability_dist = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_letter_guess = torch.multinomial(next_letter_probability_dist, 1)\n",
    "            x = torch.cat((x, next_letter_guess), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "xb, yb = get_batch(batch_size, block_size, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.1412, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5886, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.5640, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4765, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.2576, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3373, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3536, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.3237, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "BAY:\n",
      "Wa wher fir\n",
      "Thy achas toust, for,\n",
      "igh's VINIGLINI: you hisneramt.\n",
      "\n",
      "MYC:\n",
      "Ky is thisth stherem yould. GwpY:\n",
      "O the:\n",
      "ford I lich gre thavis hal at ine, mis, thegiv for''tast thout\n",
      "Kond shis! to-tepst suld ye went ofredeard wom:\n",
      "Jrung bror wendce\n",
      "Ist younthee titnest sthing dir sace acovestlet wit? my,, anf wkokis youlve fors of:\n",
      "Theml:\n",
      "Mor athe cin therit fave it to!\n",
      "Whaloomard thal tevee.\n",
      "\n",
      "NETIE:\n",
      "An, proys theov ongisthy shat so to pins,\n",
      "tholw nodere wantill wand morgh' itnou pysmes pot thim t\n"
     ]
    }
   ],
   "source": [
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "for step in range(max_iters):\n",
    "    xb, yb = get_batch(batch_size, block_size, train)\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 500 == 0:\n",
    "        print('Loss:',loss)\n",
    "\n",
    "print('Loss:', loss)\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "print(decode(model.generate(context, how_many_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
